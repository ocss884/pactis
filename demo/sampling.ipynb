{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junix/PyProject/.base3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from torch.nn import functional as F\n",
    "\n",
    "path = Path(os.path.abspath(os.curdir)).parent.parent\n",
    "import sys\n",
    "sys.path.append(str(path))\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "from pactis.model.core.adapter import InputAdapter\n",
    "from pactis.model.core.config import LatentQueryConfig, CrossAttentionLayerConfig, SelfAttentionBlockConfig, PerceiverEncoderConfig, PerceiverDecoderConfig\n",
    "from pactis.model.core.encoder import PerceiverIO, PerceiverEncoder, PerceiverDecoder\n",
    "from pactis.model.core.decoder import AttentionalCopula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EncoderConfig = PerceiverEncoderConfig.create_from_config(InputAdapter(128),\n",
    "                                                        LatentQueryConfig(num_latents=20,\n",
    "                                                                        num_latent_dim=64),\n",
    "                                                        CrossAttentionLayerConfig(num_heads=8,\n",
    "                                                                                  num_q_input_dim=64,\n",
    "                                                                                  num_kv_input_dim=128,\n",
    "                                                                                  num_qk_dim=64,\n",
    "                                                                                  num_v_dim=64),\n",
    "                                                        SelfAttentionBlockConfig(num_layers=4,\n",
    "                                                                                 num_heads=8,\n",
    "                                                                                 num_dim=64),\n",
    "                                                        num_cross_attn_layers=1,\n",
    "                                                        num_self_attn_blocks=8,\n",
    "                                                        )\n",
    "DecoderConfig = PerceiverDecoderConfig.create_from_config(LatentQueryConfig(100, 128), \n",
    "                                                        CrossAttentionLayerConfig(8, 128, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = PerceiverEncoder.from_config(EncoderConfig).to(\"cuda\")\n",
    "Decoder = PerceiverDecoder.from_config(DecoderConfig).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 100, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "optimizer = torch.optim.Adam(list(Encoder.parameters()) + list(Decoder.parameters()), lr=1e-3)\n",
    "x = torch.randn(200, 100, 128, device='cuda')\n",
    "# with torch.no_grad():\n",
    "for i in range(50):\n",
    "    x = Encoder(x)\n",
    "    x = x.repeat(1, 5, 2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., inf, inf, inf, inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., inf, inf, inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., inf, inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., inf]],\n",
       "\n",
       "        [[0., 0., 0., 0., 0., 0., inf, inf, inf, inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., inf, inf, inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., inf, inf],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., inf]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(float(\"inf\")*torch.ones(2, 4, 10)).flip(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
       "         [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
       "         [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
       "         [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]],\n",
       "\n",
       "        [[inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
       "         [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
       "         [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf],\n",
       "         [inf, inf, inf, inf, inf, inf, inf, inf, inf, inf]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AttentionalCopulaConfig:\n",
    "    input_dim: int = 15\n",
    "    attn_heads: int = 5\n",
    "    attn_dim: int = 16\n",
    "    attn_layers: int = 3\n",
    "    mlp_dim: int = 32\n",
    "    mlp_layers: int = 3\n",
    "    resolution: int = 10\n",
    "\n",
    "    @property\n",
    "    def dict(self):\n",
    "        return asdict(self)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = AttentionalCopulaConfig()\n",
    "model = AttentionalCopula(**config.dict).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = torch.randn((100, 5, 20, 15), device=device)\n",
    "true_u = torch.rand((100, 5, 20), device=device, dtype=torch.float)\n",
    "print(true_u[0, :, 7:])\n",
    "mask = torch.Tensor([1]*8+[0]*3+[1]*9)\n",
    "print(model.sample(encoded, true_u, mask, device=device)[0, :, 7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.MultiheadAttention(embed_dim=15, num_heads=5, kdim=32, vdim=64, batch_first=True)\n",
    "for module in model.named_parameters():\n",
    "    print(module[0], module[1].shape)\n",
    "model(torch.randn(10, 100, 15), torch.randn(10, 100, 32), torch.randn(10, 100, 64))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((torch.randn((9, 5, 20, 15))[:,:0,:,9:9+1], torch.randn((9, 5, 20, 15))[:,0:,:,9:9+1]), axis=3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(10, 50)\n",
    "A.masked_fill_(, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = -torch.finfo(torch.float32).max\n",
    "A = torch.Tensor([A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000e+00,  1.0000e+00, -3.4028e+38])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([1, 1, -torch.finfo(torch.float32).max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = torch.Tensor([float(\"-inf\")])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pactis.model.core.modules import MultiHeadAttention\n",
    "model = MultiHeadAttention(5, 15, 20)\n",
    "model(torch.randn(1, 50, 15), torch.randn(1, 100, 20), pad_mask=torch.Tensor([[1]*99+[0]*1]).bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.LayerNorm((50))(torch.rand(2,3,50))[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(nn.Sequential):\n",
    "    def forward(self, *x, **kwargs):\n",
    "        for i, module in enumerate(self):\n",
    "            if type(x) == tuple:\n",
    "                if i == 0:\n",
    "                    x = module(*x, **kwargs)\n",
    "                else:\n",
    "                    x = module(*x)\n",
    "            else:\n",
    "                x = module(x)\n",
    "        return x\n",
    "class mdl(Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__(nn.Linear(5, 10), nn.Linear(10, 15))\n",
    "model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from torch.nn import functional as F\n",
    "\n",
    "path = Path(os.path.abspath(os.curdir)).parent.parent\n",
    "import sys\n",
    "sys.path.append(str(path))\n",
    "\n",
    "from pactis.model.core.decoder import AttentionalCopula\n",
    "from pactis.model.core.modules import CrossAttentionLayer, Sequential, Residual\n",
    "model = CrossAttentionLayer(5, 15, 20)\n",
    "model(torch.randn(1, 50, 15), torch.randn(1, 100, 20), torch.randn(1, 100, 20)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(nn.Sequential):\n",
    "    def forward(self, *x, **kwargs):\n",
    "        for i, module in enumerate(self):\n",
    "            if type(x) == tuple:\n",
    "                if i == 0:\n",
    "                    x = module(*x, **kwargs)\n",
    "                else:\n",
    "                    x = module(*x)\n",
    "            else:\n",
    "                x = module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MultiheadAttention\n",
    "model = MultiheadAttention(15, 5, kdim=10, vdim=20)\n",
    "MODEL = Sequential(model, nn.Linear(15, 64))\n",
    "MODEL(torch.randn(1, 100, 15), torch.randn(1, 100, 10), torch.randn(1, 100, 20))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from dataclasses import dataclass, asdict, KW_ONLY, field\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    @property\n",
    "    def dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "@dataclass\n",
    "class AConfig(Config):\n",
    "    a: int = 10\n",
    "    b: int = 20\n",
    "    _: KW_ONLY\n",
    "    c: int = 30\n",
    "    d: int = field(init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d = self.a + self.b\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CrossAttentionLayerConfig(Config):\n",
    "    n_heads: int\n",
    "    num_q_input_dim: int\n",
    "    num_kv_input_dim: int\n",
    "    num_qk_dim: Optional[int] = None\n",
    "    num_v_dim: Optional[int] = None\n",
    "    qkv_bias: bool = True\n",
    "    out_bias: bool = True\n",
    "    mlp_bias: bool = True\n",
    "    widening_factor: int = 1\n",
    "    dropout: float = 0.1\n",
    "    batch_first: bool = True\n",
    "    norm_first: bool = True\n",
    "    device: Optional[torch.device] = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype: Optional[torch.dtype] = None\n",
    "\n",
    "config = CrossAttentionLayerConfig(5, 15, 20)\n",
    "config.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CLASS:\n",
    "    a: int = 10\n",
    "    b: int = 20\n",
    "    c: int = 30\n",
    "    d: int = field(init=False)\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d = self.a + self.b\n",
    "        \n",
    "    @classmethod\n",
    "    def from_else(cls, a, b, c):\n",
    "        return cls(a, b, c)\n",
    "\n",
    "# MODEL = CLASS()\n",
    "MODEL = CLASS.from_else(1, 2, 3)\n",
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    _: KW_ONLY\n",
    "    device: Optional[torch.device] = None\n",
    "    dtype: Optional[torch.dtype] = None\n",
    "    @property\n",
    "    def dict(self):\n",
    "        return asdict(self)\n",
    "@dataclass\n",
    "class batch_norm_order:\n",
    "    _: KW_ONLY\n",
    "    norm_first: bool = True\n",
    "    batch_first: bool = True\n",
    "\n",
    "@dataclass\n",
    "class LatentQueryConfig(Config, batch_norm_order):\n",
    "    num_latents: int\n",
    "    num_latent_dim: int\n",
    "    init_scale: float = 0.02\n",
    "config = LatentQueryConfig(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pactis.model.core.modules import SelfAttentionBlock\n",
    "# from pactis.model.core.encoder import PerceiverEncoder, PerceiverDecoder\n",
    "from pactis.model.core.config import PerceiverEncoderConfig, PerceiverDecoderConfig\n",
    "from pactis.model.core.adapter import InputAdapter\n",
    "# config = SelfAttentionBlockConfig(5, 15, dropout=0.1, device=\"cuda\")\n",
    "# model = SelfAttentionBlock(**config.dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junix/PyProject/.base3.10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.Tensor([1, 1, float(\"inf\")]).masked_fill_(torch.Tensor([1, 1, float(\"inf\")]) == float(\"inf\"), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000, 0.0000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([1, 1, -float(\"inf\")]).softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerceiverEncoder(\n",
       "  (latent_provider): LatentQuery()\n",
       "  (input_adapter): InputAdapter()\n",
       "  (cross_attn_1): CrossAttentionLayer(\n",
       "    (cross_attn): CrossAttention(\n",
       "      (attention): MultiHeadAttention(\n",
       "        (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (k_proj): Linear(in_features=15, out_features=16, bias=True)\n",
       "        (v_proj): Linear(in_features=15, out_features=16, bias=True)\n",
       "        (o_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (kv_norm): LayerNorm((15,), eps=1e-05, elementwise_affine=True)\n",
       "    (attn_out_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "    (_ca_layer): Sequential(\n",
       "      (0): Residual(\n",
       "        (module): CrossAttention(\n",
       "          (attention): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (k_proj): Linear(in_features=15, out_features=16, bias=True)\n",
       "            (v_proj): Linear(in_features=15, out_features=16, bias=True)\n",
       "            (o_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Residual(\n",
       "        (module): Sequential(\n",
       "          (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Sequential(\n",
       "            (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (self_attn_1): SelfAttentionBlock(\n",
       "    (0): SelfAttentionLayer(\n",
       "      (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Residual(\n",
       "        (module): SelfAttention(\n",
       "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (o_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Residual(\n",
       "        (module): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): SelfAttentionLayer(\n",
       "      (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Residual(\n",
       "        (module): SelfAttention(\n",
       "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (o_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Residual(\n",
       "        (module): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SelfAttentionLayer(\n",
       "      (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Residual(\n",
       "        (module): SelfAttention(\n",
       "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (o_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Residual(\n",
       "        (module): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): SelfAttentionLayer(\n",
       "      (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Residual(\n",
       "        (module): SelfAttention(\n",
       "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (o_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Residual(\n",
       "        (module): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): SelfAttentionLayer(\n",
       "      (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Residual(\n",
       "        (module): SelfAttention(\n",
       "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (o_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Residual(\n",
       "        (module): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): SelfAttentionLayer(\n",
       "      (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Residual(\n",
       "        (module): SelfAttention(\n",
       "          (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (k_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (v_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (o_proj): Linear(in_features=16, out_features=16, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Residual(\n",
       "        (module): Sequential(\n",
       "          (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=16, out_features=16, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = PerceiverEncoderConfig(InputAdapter(15, 20), 5, 16)\n",
    "PerceiverEncoder(**config.dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".base3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b46e680af6c1982026ac639fc2d3cf57b97ed93ae92b2cb694c5e34d7389b6d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
